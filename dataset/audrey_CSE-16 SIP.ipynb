{"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30580,"status":"ok","timestamp":1708226808966,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"z5qWsass6m3h","outputId":"485a4fcd-ad2c-4dee-84aa-2acd1b9ec54c"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'torch-cam' already exists and is not an empty directory.\n","Obtaining file:///content/torch-cam\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcam==0.4.1.dev0) (2.1.0+cu121)\n","Requirement already satisfied: numpy<2.0.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchcam==0.4.1.dev0) (1.25.2)\n","Requirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from torchcam==0.4.1.dev0) (9.4.0)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from torchcam==0.4.1.dev0) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam==0.4.1.dev0) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->torchcam==0.4.1.dev0) (1.3.0)\n","Building wheels for collected packages: torchcam\n","  Building editable for torchcam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchcam: filename=torchcam-0.4.1.dev0-0.editable-py3-none-any.whl size=16317 sha256=5a5c33b77c6377ea0ea2f8a436eb4fe2fae247870f929f9b9aff4d2189920e36\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-r34rko46/wheels/01/be/d9/bcd58c55f0941c76ea49be4bc6b7a29cbf46c236c38c1b3b98\n","Successfully built torchcam\n","Installing collected packages: torchcam\n","  Attempting uninstall: torchcam\n","    Found existing installation: torchcam 0.4.1.dev0\n","    Uninstalling torchcam-0.4.1.dev0:\n","      Successfully uninstalled torchcam-0.4.1.dev0\n","Successfully installed torchcam-0.4.1.dev0\n","Requirement already satisfied: grad-cam in /usr/local/lib/python3.10/dist-packages (1.5.0)\n","Requirement already satisfied: torchcam in /usr/local/lib/python3.10/dist-packages (0.4.1.dev0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.25.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (9.4.0)\n","Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.16.0+cu121)\n","Requirement already satisfied: ttach in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.8.0.76)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.2.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->grad-cam) (2.31.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n","Requirement already satisfied: torchcam in /usr/local/lib/python3.10/dist-packages (0.4.1.dev0)\n","Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (2.1.0+cu121)\n","Requirement already satisfied: numpy<2.0.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchcam) (1.25.2)\n","Requirement already satisfied: Pillow!=9.2.0,>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (9.4.0)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from torchcam) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.0->torchcam) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->torchcam) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.0->torchcam) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->torchcam) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->torchcam) (1.3.0)\n"]}],"source":["!git clone https://github.com/frgfm/torch-cam.git\n","!pip install -e torch-cam/.\n","!pip install grad-cam torchcam\n","!pip install torchcam"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20815,"status":"ok","timestamp":1708225658229,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"5IAiHaLnGe4A","outputId":"09cc9491-edfb-4a10-d13c-a3bb70e59102"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"A4b0Ave2HVoh","executionInfo":{"status":"ok","timestamp":1708225658230,"user_tz":480,"elapsed":12,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["data_dir = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/audrey_dataset_plants'\n","train_dir = data_dir + '/train'\n","test_dir = data_dir + '/test'"]},{"cell_type":"markdown","metadata":{"id":"sxbyDf4aH4m1"},"source":["Import"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"UwKqC40bH3gx","executionInfo":{"status":"ok","timestamp":1708225670473,"user_tz":480,"elapsed":12253,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import cv2\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models"]},{"cell_type":"markdown","metadata":{"id":"ngcd1TRxIXuL"},"source":["Set device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2s0keqquJLxJ"},"outputs":[],"source":["# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DnXad8JNIRJ7","executionInfo":{"status":"ok","timestamp":1708225670475,"user_tz":480,"elapsed":89,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["def set_device():\n","  if torch.cuda.is_available():\n","    dev = \"cuda:0\"\n","  else:\n","    dev = \"cpu\"\n","  return torch.device(dev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBZyJxIQI9oG"},"outputs":[],"source":["# print('device', device)"]},{"cell_type":"markdown","metadata":{"id":"uBo-N206J0A3"},"source":["Load pretrained model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fG32-Y5rJzX-","executionInfo":{"status":"ok","timestamp":1708225942984,"user_tz":480,"elapsed":221,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from torchvision import models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3j_UOFi8J5H6"},"outputs":[],"source":["# model = models.resnet18(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CFEPx0uK5N_"},"outputs":[],"source":["# model = model.eval()\n","# model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"o9N59NzIbPKn"},"source":["Get data to normalize"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"JNKBQwedbO9T","executionInfo":{"status":"ok","timestamp":1708225944644,"user_tz":480,"elapsed":221,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from torchvision import transforms\n","from torchvision import datasets"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"S7zZaRQobO0V","executionInfo":{"status":"ok","timestamp":1708225947504,"user_tz":480,"elapsed":1507,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["# Get the data from your train dataset for getting the mean and std\n","train_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n","                                       transforms.ToTensor()])\n","\n","training_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"l1p5p8R0bOrG","executionInfo":{"status":"ok","timestamp":1708225948340,"user_tz":480,"elapsed":181,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["def get_mean_and_std(loader):\n","    mean = torch.zeros(3)\n","    std = torch.zeros(3)\n","    count = 0\n","\n","    # Loop over the images in the DataLoader\n","    for data in train_loader:\n","        # Apply the transformation to the image\n","        img = data[0]\n","\n","\n","        # Compute the mean and standard deviation of the pixel values in the image\n","        mean += torch.mean(img,dim=(0,2,3))\n","        std += torch.std(img,dim=(0,2,3))\n","        count += 1\n","\n","    # Compute the mean and standard deviation across all images\n","    mean /= count\n","    std /= count\n","\n","    print('Mean:', mean)\n","    print('Std:', std)\n","    return mean,std"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsAWmXmEbOgN"},"outputs":[],"source":["# after run this for one time, just record this data and comment out this part\n","get_mean_and_std(train_loader)"]},{"cell_type":"markdown","metadata":{"id":"uF1yze_HL35Y"},"source":["Preprocess data"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hjzIXEjPL3HM","executionInfo":{"status":"ok","timestamp":1708225953257,"user_tz":480,"elapsed":182,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from torchvision import transforms\n","from torchvision import datasets"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"gqBA934lL_JN","executionInfo":{"status":"ok","timestamp":1708225954845,"user_tz":480,"elapsed":668,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["# set up the transforms for the datasets\n","train_transforms = transforms.Compose([transforms.RandomRotation(30),\n","                                       transforms.RandomResizedCrop(224),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.5188, 0.4596, 0.3459],\n","                                                            [0.2735, 0.2508, 0.2684])])\n","test_transforms = transforms.Compose([transforms.Resize(256),\n","                                      transforms.CenterCrop(224),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.5188, 0.4596, 0.3459],\n","                                                           [0.2735, 0.2508, 0.2684])])\n","\n","training_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n","test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"uIOTkWtWO1a3"},"source":["Load the json file"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1708225956572,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"lTdPQhjPgLCE","outputId":"d05b7499-5af8-4a47-f96a-9f3c7af01b59"},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","{'0': 'dandelion', '1': 'poison ivy', '2': 'tulip'}\n"]}],"source":["import json\n","\n","with open('/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/audrey_dataset_plants/flower_types.json', 'r') as f:\n","  classes_name = json.load(f)\n","\n","print(len(classes_name))\n","print(classes_name)"]},{"cell_type":"markdown","metadata":{"id":"Qu44x0GG7L5b"},"source":["Training functions"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"KZi85dJ36S3w","executionInfo":{"status":"ok","timestamp":1708225958656,"user_tz":480,"elapsed":185,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs):\n","  device = set_device()\n","  best_acc = 0\n","\n","  for epoch in range(n_epochs):\n","    print(\"Epoch number %d\" % (epoch + 1))\n","    model.train()\n","    running_loss = 0.0\n","    running_correct = 0.0\n","    total = 0\n","\n","    for data in train_loader:\n","      images, labels = data\n","      images, labels = images.to(device), labels.to(device)\n","      total += labels.size(0)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = model(images)\n","\n","      _, predicted = torch.max(outputs.data, 1)\n","\n","      loss = criterion(outputs, labels)\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      running_correct += (labels==predicted).sum().item()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_acc = 100.00 * running_correct / total\n","\n","    print(\"     = Training dataset. Got %d out of %d images correctly (%.3f%%). Epoch loss: %.3f\"\n","          % (running_correct, total, epoch_acc, epoch_loss))\n","\n","    test_dataset_acc = evaluate_model_on_test_set(model, test_loader)\n","\n","    if (test_dataset_acc > best_acc):\n","      best_acc = test_dataset_acc\n","      save_checkpoint(model, epoch, best_acc)\n","\n","  print(\"Finished\")\n","  return model"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Q3P6J9ZD6br0","executionInfo":{"status":"ok","timestamp":1708225961149,"user_tz":480,"elapsed":174,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["def evaluate_model_on_test_set(model, test_loader):\n","  model.eval()\n","  predicted_correctly_on_epoch = 0\n","  total = 0\n","  device = set_device()\n","\n","  with torch.no_grad():\n","    for data in test_loader:\n","      images, labels = data\n","      images, labels = images.to(device), labels.to(device)\n","      total += labels.size(0)\n","\n","      outputs = model(images)\n","\n","      _, predicted = torch.max(outputs.data, 1)\n","\n","      predicted_correctly_on_epoch += (predicted==labels).sum().item()\n","\n","  epoch_acc = 100.00 * predicted_correctly_on_epoch / total\n","  print(\"     = Test dataset. Got %d out of %d images correctly (%.3f%%)\"\n","        % (predicted_correctly_on_epoch, total, epoch_acc))\n","\n","  return epoch_acc"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BmIBczdh6fva","executionInfo":{"status":"ok","timestamp":1708225963157,"user_tz":480,"elapsed":183,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["def save_checkpoint(model, epoch, best_acc):\n","  state = {\n","      'epoch' : epoch + 1,\n","      'model' : model.state_dict(),\n","      'best accuracy' : best_acc,\n","  }\n","  torch.save(state, 'model_best_checkpoint.pth.tar')"]},{"cell_type":"markdown","metadata":{"id":"YQ8o2y2f6lE1"},"source":["Set up training"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":828,"status":"ok","timestamp":1708225965697,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"q-UBaCM76kiT","outputId":"8aa4101f-002e-4470-9ae9-4493e757e83b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","resnet18_model = models.resnet18(pretrained=False)\n","num_ftrs = resnet18_model.fc.in_features\n","number_of_classes = 3\n","resnet18_model.fc = nn.Linear(num_ftrs, number_of_classes)\n","device = set_device()\n","resnet_18_model = resnet18_model.to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer = optim.SGD(resnet18_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.003)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261244,"status":"ok","timestamp":1708226231216,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"Wd2hFyWP6srR","outputId":"e2826b9b-d8c6-4bc3-df1e-96853d9b348b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch number 1\n","     = Training dataset. Got 24 out of 87 images correctly (27.586%). Epoch loss: 1.206\n","     = Test dataset. Got 12 out of 37 images correctly (32.432%)\n","Epoch number 2\n","     = Training dataset. Got 46 out of 87 images correctly (52.874%). Epoch loss: 1.041\n","     = Test dataset. Got 15 out of 37 images correctly (40.541%)\n","Epoch number 3\n","     = Training dataset. Got 47 out of 87 images correctly (54.023%). Epoch loss: 0.918\n","     = Test dataset. Got 16 out of 37 images correctly (43.243%)\n","Epoch number 4\n","     = Training dataset. Got 72 out of 87 images correctly (82.759%). Epoch loss: 0.644\n","     = Test dataset. Got 12 out of 37 images correctly (32.432%)\n","Epoch number 5\n","     = Training dataset. Got 67 out of 87 images correctly (77.011%). Epoch loss: 0.537\n","     = Test dataset. Got 17 out of 37 images correctly (45.946%)\n","Epoch number 6\n","     = Training dataset. Got 69 out of 87 images correctly (79.310%). Epoch loss: 0.641\n","     = Test dataset. Got 18 out of 37 images correctly (48.649%)\n","Epoch number 7\n","     = Training dataset. Got 74 out of 87 images correctly (85.057%). Epoch loss: 0.485\n","     = Test dataset. Got 17 out of 37 images correctly (45.946%)\n","Epoch number 8\n","     = Training dataset. Got 75 out of 87 images correctly (86.207%). Epoch loss: 0.676\n","     = Test dataset. Got 20 out of 37 images correctly (54.054%)\n","Epoch number 9\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.227\n","     = Test dataset. Got 16 out of 37 images correctly (43.243%)\n","Epoch number 10\n","     = Training dataset. Got 72 out of 87 images correctly (82.759%). Epoch loss: 0.519\n","     = Test dataset. Got 15 out of 37 images correctly (40.541%)\n","Epoch number 11\n","     = Training dataset. Got 78 out of 87 images correctly (89.655%). Epoch loss: 0.303\n","     = Test dataset. Got 13 out of 37 images correctly (35.135%)\n","Epoch number 12\n","     = Training dataset. Got 72 out of 87 images correctly (82.759%). Epoch loss: 0.512\n","     = Test dataset. Got 14 out of 37 images correctly (37.838%)\n","Epoch number 13\n","     = Training dataset. Got 78 out of 87 images correctly (89.655%). Epoch loss: 0.368\n","     = Test dataset. Got 15 out of 37 images correctly (40.541%)\n","Epoch number 14\n","     = Training dataset. Got 78 out of 87 images correctly (89.655%). Epoch loss: 0.339\n","     = Test dataset. Got 21 out of 37 images correctly (56.757%)\n","Epoch number 15\n","     = Training dataset. Got 78 out of 87 images correctly (89.655%). Epoch loss: 0.240\n","     = Test dataset. Got 22 out of 37 images correctly (59.459%)\n","Epoch number 16\n","     = Training dataset. Got 77 out of 87 images correctly (88.506%). Epoch loss: 0.392\n","     = Test dataset. Got 25 out of 37 images correctly (67.568%)\n","Epoch number 17\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.288\n","     = Test dataset. Got 25 out of 37 images correctly (67.568%)\n","Epoch number 18\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.270\n","     = Test dataset. Got 23 out of 37 images correctly (62.162%)\n","Epoch number 19\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.202\n","     = Test dataset. Got 24 out of 37 images correctly (64.865%)\n","Epoch number 20\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.223\n","     = Test dataset. Got 25 out of 37 images correctly (67.568%)\n","Epoch number 21\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.216\n","     = Test dataset. Got 28 out of 37 images correctly (75.676%)\n","Epoch number 22\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.115\n","     = Test dataset. Got 27 out of 37 images correctly (72.973%)\n","Epoch number 23\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.217\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 24\n","     = Training dataset. Got 85 out of 87 images correctly (97.701%). Epoch loss: 0.129\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 25\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.159\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 26\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.178\n","     = Test dataset. Got 30 out of 37 images correctly (81.081%)\n","Epoch number 27\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.122\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 28\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.170\n","     = Test dataset. Got 28 out of 37 images correctly (75.676%)\n","Epoch number 29\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.230\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 30\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.250\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 31\n","     = Training dataset. Got 76 out of 87 images correctly (87.356%). Epoch loss: 0.266\n","     = Test dataset. Got 28 out of 37 images correctly (75.676%)\n","Epoch number 32\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.157\n","     = Test dataset. Got 23 out of 37 images correctly (62.162%)\n","Epoch number 33\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.298\n","     = Test dataset. Got 24 out of 37 images correctly (64.865%)\n","Epoch number 34\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.165\n","     = Test dataset. Got 25 out of 37 images correctly (67.568%)\n","Epoch number 35\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.403\n","     = Test dataset. Got 31 out of 37 images correctly (83.784%)\n","Epoch number 36\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.214\n","     = Test dataset. Got 28 out of 37 images correctly (75.676%)\n","Epoch number 37\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.227\n","     = Test dataset. Got 27 out of 37 images correctly (72.973%)\n","Epoch number 38\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.206\n","     = Test dataset. Got 24 out of 37 images correctly (64.865%)\n","Epoch number 39\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.293\n","     = Test dataset. Got 22 out of 37 images correctly (59.459%)\n","Epoch number 40\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.369\n","     = Test dataset. Got 24 out of 37 images correctly (64.865%)\n","Epoch number 41\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.335\n","     = Test dataset. Got 25 out of 37 images correctly (67.568%)\n","Epoch number 42\n","     = Training dataset. Got 77 out of 87 images correctly (88.506%). Epoch loss: 0.302\n","     = Test dataset. Got 30 out of 37 images correctly (81.081%)\n","Epoch number 43\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.113\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 44\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.110\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 45\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.318\n","     = Test dataset. Got 30 out of 37 images correctly (81.081%)\n","Epoch number 46\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.163\n","     = Test dataset. Got 26 out of 37 images correctly (70.270%)\n","Epoch number 47\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.142\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 48\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.311\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 49\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.110\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 50\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.310\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 51\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.192\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 52\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.091\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 53\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.106\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 54\n","     = Training dataset. Got 82 out of 87 images correctly (94.253%). Epoch loss: 0.128\n","     = Test dataset. Got 33 out of 37 images correctly (89.189%)\n","Epoch number 55\n","     = Training dataset. Got 77 out of 87 images correctly (88.506%). Epoch loss: 0.241\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 56\n","     = Training dataset. Got 81 out of 87 images correctly (93.103%). Epoch loss: 0.123\n","     = Test dataset. Got 31 out of 37 images correctly (83.784%)\n","Epoch number 57\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.180\n","     = Test dataset. Got 31 out of 37 images correctly (83.784%)\n","Epoch number 58\n","     = Training dataset. Got 79 out of 87 images correctly (90.805%). Epoch loss: 0.261\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 59\n","     = Training dataset. Got 85 out of 87 images correctly (97.701%). Epoch loss: 0.155\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 60\n","     = Training dataset. Got 85 out of 87 images correctly (97.701%). Epoch loss: 0.084\n","     = Test dataset. Got 35 out of 37 images correctly (94.595%)\n","Epoch number 61\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.134\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 62\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.114\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 63\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.111\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 64\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.206\n","     = Test dataset. Got 34 out of 37 images correctly (91.892%)\n","Epoch number 65\n","     = Training dataset. Got 80 out of 87 images correctly (91.954%). Epoch loss: 0.199\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Epoch number 66\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.070\n","     = Test dataset. Got 31 out of 37 images correctly (83.784%)\n","Epoch number 67\n","     = Training dataset. Got 85 out of 87 images correctly (97.701%). Epoch loss: 0.140\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 68\n","     = Training dataset. Got 83 out of 87 images correctly (95.402%). Epoch loss: 0.115\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 69\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.100\n","     = Test dataset. Got 29 out of 37 images correctly (78.378%)\n","Epoch number 70\n","     = Training dataset. Got 84 out of 87 images correctly (96.552%). Epoch loss: 0.124\n","     = Test dataset. Got 32 out of 37 images correctly (86.486%)\n","Finished\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":17}],"source":["train_nn(resnet18_model, train_loader, test_loader, loss_fn, optimizer, n_epochs=70)"]},{"cell_type":"markdown","metadata":{"id":"Egiy6esl4l-Q"},"source":["Load checkpoint"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cQ0chyPl4q1M","executionInfo":{"status":"ok","timestamp":1708226231217,"user_tz":480,"elapsed":11,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["checkpoint = torch.load('model_best_checkpoint.pth.tar')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708226231217,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"tJeB0bYO4tEr","outputId":"72ad4fb9-5890-48c0-fa3d-3a5eb4b64096"},"outputs":[{"output_type":"stream","name":"stdout","text":["60\n","94.5945945945946\n"]}],"source":["print(checkpoint['epoch'])\n","print(checkpoint['best accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"SLvTWX245CC9"},"source":["Save model"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"0n9Oy3Jk5BNy","executionInfo":{"status":"ok","timestamp":1708226231460,"user_tz":480,"elapsed":246,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["resnet18_model = models.resnet18()\n","num_ftrs = resnet18_model.fc.in_features\n","number_of_classes = 3\n","resnet18_model.fc = nn.Linear(num_ftrs, number_of_classes)\n","resnet18_model.load_state_dict(checkpoint['model'])\n","\n","torch.save(resnet18_model, 'best_model.pth')"]},{"cell_type":"markdown","metadata":{"id":"9A1THNL45alB"},"source":["Testing for one picture"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"mwM-g99F5c3W","executionInfo":{"status":"ok","timestamp":1708226231461,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["classes = [\n","    \"dandelion\",\n","    \"poison ivy\",\n","    \"tulips\"\n","]"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"iiCfwIEf5pv1","executionInfo":{"status":"ok","timestamp":1708226231462,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["model = torch.load('best_model.pth')"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"u7wQzPTf5vGk","executionInfo":{"status":"ok","timestamp":1708226231462,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["image_transforms = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Normalize([0.5188, 0.4596, 0.3459],\n","                                                           [0.2735, 0.2508, 0.2684])])"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"wJ5uhsCU6BGq","executionInfo":{"status":"ok","timestamp":1708226231691,"user_tz":480,"elapsed":233,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["import PIL.Image as Image\n","\n","def classify(model, image_transforms, image_path, classes):\n","  model = model.eval()\n","  image = Image.open(image_path)\n","  image = image_transforms(image).float()\n","  image = image.unsqueeze(0)\n","\n","  output = model(image)\n","  _, predicted = torch.max(output.data, 1)\n","\n","  print(classes[predicted.item()])"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":749,"status":"ok","timestamp":1708226232438,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"XugpIPNC6QzV","outputId":"cf783000-d34c-4c50-9559-11bcc797ffef"},"outputs":[{"output_type":"stream","name":"stdout","text":["tulips\n"]}],"source":["classify (model, image_transforms, \"/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/audrey_dataset_plants/vd1t.webp\", classes)"]},{"cell_type":"markdown","metadata":{"id":"MZKTsgNOyl7R"},"source":["CAM (Class Activated Map)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"zJ1dwbAMWeZR","executionInfo":{"status":"ok","timestamp":1708226232439,"user_tz":480,"elapsed":13,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["# from torchvision.models import resnet18\n","# model = resnet18(pretrained=True).eval().to(device)\n","model = torch.load('best_model.pth')\n","model = model.eval().to(device)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"executionInfo":{"elapsed":5,"status":"error","timestamp":1708226825468,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"},"user_tz":480},"id":"9LDmovvI46RH","outputId":"197f30bd-1e84-45e9-9b2b-e97ba00ba570"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torchcam'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-e24039099876>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSmoothGradCAMpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcam_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmoothGradCAMpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcam'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from torchcam.methods import SmoothGradCAMpp\n","\n","cam_extractor = SmoothGradCAMpp(model)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"UUBAc6-L48-R","executionInfo":{"status":"ok","timestamp":1708226829375,"user_tz":480,"elapsed":174,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["test_transform = transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     transforms.Normalize(\n","                                         mean=[0.5188, 0.4596, 0.3459],\n","                                         std=[0.2735, 0.2508, 0.2684])\n","                                    ])\n","\n","\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"WdP3eKj-5AvH","executionInfo":{"status":"ok","timestamp":1708226831177,"user_tz":480,"elapsed":3,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["img_path = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/audrey_dataset_plants/vd1t.webp'"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"G7bECA1w5Bjk","executionInfo":{"status":"ok","timestamp":1708226832160,"user_tz":480,"elapsed":166,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["img_pil = Image.open(img_path)\n","input_tensor = test_transform(img_pil).unsqueeze(0).to(device)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"V4_kSRTq5KDt","executionInfo":{"status":"ok","timestamp":1708226835426,"user_tz":480,"elapsed":368,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["pred_logits = model(input_tensor)\n","pred_id = torch.topk(pred_logits, 1)[1].detach().cpu().numpy().squeeze().item()"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"e8V3AJah5M6H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708226836553,"user_tz":480,"elapsed":161,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}},"outputId":"5b3a3938-6164-454c-d73e-5cf40f8ee0db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":39}],"source":["pred_id"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"xsh-t-Pq5PFP","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"error","timestamp":1708226837647,"user_tz":480,"elapsed":6,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}},"outputId":"e5870258-cdbd-42d1-e156-10b7f8ca22cf"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'cam_extractor' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-8c3078e2b7a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactivation_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mactivation_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cam_extractor' is not defined"]}],"source":["activation_map = cam_extractor(pred_id, pred_logits)\n","activation_map = activation_map[0][0].detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbiZrcQI5R8g","executionInfo":{"status":"aborted","timestamp":1708226232442,"user_tz":480,"elapsed":11,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["activation_map.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VAmmw6V5UNV","executionInfo":{"status":"aborted","timestamp":1708226232443,"user_tz":480,"elapsed":12,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["plt.imshow(activation_map)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Leb9ebbs5YAu","executionInfo":{"status":"aborted","timestamp":1708226232443,"user_tz":480,"elapsed":152678,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from torchcam.utils import overlay_mask\n","\n","result = overlay_mask(img_pil, Image.fromarray(activation_map), alpha=0.6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqRIizRFKePW","executionInfo":{"status":"aborted","timestamp":1708226232444,"user_tz":480,"elapsed":151714,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["result"]},{"cell_type":"markdown","metadata":{"id":"TfGRzECtpypk"},"source":["Grad cam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8JV75SVpzS-","executionInfo":{"status":"aborted","timestamp":1708226232444,"user_tz":480,"elapsed":131838,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from pytorch_grad_cam import GradCAM\n","# HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, EigenGradCAM, LayerCAM, FullGrad, GradCAMElementWise\n","from pytorch_grad_cam import GuidedBackpropReLUModel\n","from pytorch_grad_cam.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","\n","model = torch.load('best_model.pth')\n","model = model.eval().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjHvJ882pzPw","executionInfo":{"status":"aborted","timestamp":1708226232444,"user_tz":480,"elapsed":130899,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["from torchvision import transforms\n","\n","test_transform = transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     transforms.Normalize(\n","                                         mean=[0.5188, 0.4596, 0.3459],\n","                                         std=[0.2735, 0.2508, 0.2684])\n","                                    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvsiCcwzpzMA","executionInfo":{"status":"aborted","timestamp":1708226232445,"user_tz":480,"elapsed":129155,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["img_path = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/audrey_dataset_plants/vd1t.webp'\n","\n","img_pil = Image.open(img_path)\n","input_tensor = test_transform(img_pil).unsqueeze(0).to(device)\n","input_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJirLAM7pzCT","executionInfo":{"status":"aborted","timestamp":1708226232445,"user_tz":480,"elapsed":128191,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["model.layer4[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dG9Ed0rpy3L","executionInfo":{"status":"aborted","timestamp":1708226232682,"user_tz":480,"elapsed":4,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["target_layers = [model.layer4[-1]]\n","cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjPN0a88qGS1","executionInfo":{"status":"aborted","timestamp":1708226232683,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["with GradCAM(model=model, target_layers=target_layers, use_cuda=True) as cam:\n","    cam_map = cam(input_tensor=input_tensor, targets=None, aug_smooth=True, eigen_smooth=True)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bj8mkdgKqJIB","executionInfo":{"status":"aborted","timestamp":1708226232683,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["cam_map.shape\n","plt.imshow(cam_map)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaL3vEStqL0v","executionInfo":{"status":"aborted","timestamp":1708226232683,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["import torchcam\n","from torchcam.utils import overlay_mask\n","\n","result = overlay_mask(img_pil, Image.fromarray(cam_map), alpha=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v62uzOFPqOdV","executionInfo":{"status":"aborted","timestamp":1708226232683,"user_tz":480,"elapsed":5,"user":{"displayName":"Audrey Na","userId":"16459188490385740120"}}},"outputs":[],"source":["result"]},{"cell_type":"markdown","source":["Live cam Grad cam"],"metadata":{"id":"xEou0v9pHi-B"}},{"cell_type":"code","source":["# if not using openmim, it will stuck at install mmcv-full\n","!pip install openmim\n","!mim install mmcv-full"],"metadata":{"id":"1YRQ1mzFHioY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","import shutil\n","import tempfile\n","from tqdm import tqdm\n","\n","import cv2\n","from PIL import Image\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import gc\n","\n","from PIL import Image, ImageDraw, ImageFont\n","\n","import torch\n","import torch.nn.functional as F\n","from torchvision import models\n","\n","import mmcv\n"],"metadata":{"id":"b--lIa-yHl3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run everything in the backend\n","import matplotlib\n","matplotlib.use('Agg')"],"metadata":{"id":"it0PnbIGHynR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","train_transform = transforms.Compose([transforms.Resize(256),\n","                                     transforms.CenterCrop(224),\n","                                     transforms.ToTensor(),\n","                                     transforms.Normalize(\n","                                         mean=[0.5188, 0.4596, 0.3459],\n","                                         std=[0.2735, 0.2508, 0.2684])\n","                                    ])"],"metadata":{"id":"Jco96WmtH26B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_dir = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/flower'\n","train_path = os.path.join(dataset_dir, 'train')\n","train_dataset = datasets.ImageFolder(train_path, train_transform)\n","class_names = train_dataset.classes\n","n_class = len(class_names)"],"metadata":{"id":"PeVScKL6H-8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names"],"metadata":{"id":"TZA3j2atIUIu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for make the class_name to .npy file later\n","idx_to_labels = {y:x for x,y in train_dataset.class_to_idx.items()}"],"metadata":{"id":"zwU3PCFXIUEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save('idx_to_labels.npy', idx_to_labels)\n","np.save('labels_to_idx.npy', train_dataset.class_to_idx)"],"metadata":{"id":"l9MW23LVIT43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_to_labels = np.load('idx_to_labels.npy', allow_pickle=True).item()"],"metadata":{"id":"WzvFaoAEIT09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx_to_labels"],"metadata":{"id":"DhswM7reITig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load('best_model.pth')\n","model = model.eval().to(device)"],"metadata":{"id":"VoGYn_lUITGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pred_single_frame(img, n=5):\n","\n","    img_bgr = img\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR to RGB\n","    img_pil = Image.fromarray(img_rgb) # array to pil\n","    input_img = train_transform(img_pil).unsqueeze(0).to(device) # preprocess\n","    pred_logits = model(input_img) # get all different types' logit score\n","    pred_softmax = F.softmax(pred_logits, dim=1) # do softmax on logit scores\n","\n","    top_n = torch.topk(pred_softmax, n) # get n number of results which are most possible\n","    pred_ids = top_n[1].cpu().detach().numpy().squeeze() # get the type index\n","    confs = top_n[0].cpu().detach().numpy().squeeze() # get the probability\n","\n","    # write on the graph\n","    for i in range(n):\n","        class_name = idx_to_labels[pred_ids[i]] # get the name of that type\n","        confidence = confs[i] * 100 # get the probability in %\n","        text = '{:<15} {:>.4f}'.format(class_name, confidence)\n","\n","        img_bgr = cv2.putText(img_bgr, text, (25, 50 + 40 * i), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 0, 255), 3)\n","\n","    return img_bgr, pred_softmax"],"metadata":{"id":"wbM05LijIuif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_video = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/Resources/testing video/rose.mp4'\n","output_path = '/content/drive/MyDrive/CSE-16 SIP [ Summer 2023 ]/Resources/testing video/rose_result.mp4'"],"metadata":{"id":"KWU8GGz0IufL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a temp folder for store images\n","temp_out_dir = time.strftime('%Y%m%d%H%M%S')\n","os.mkdir(temp_out_dir)\n","print('Create temp folder to store'.format(temp_out_dir))"],"metadata":{"id":"nDSNuuCkIua0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["imgs = mmcv.VideoReader(input_video)\n","\n","prog_bar = mmcv.ProgressBar(len(imgs))\n","\n","for frame_id, img in enumerate(imgs):\n","\n","    img, pred_softmax = pred_single_frame(img, n=2)\n","\n","    cv2.imwrite(f'{temp_out_dir}/{frame_id:06d}.jpg', img)\n","\n","    prog_bar.update()\n","\n","mmcv.frames2video(temp_out_dir, output_path, fps=imgs.fps, fourcc='mp4v')\n","\n","shutil.rmtree(temp_out_dir)\n","print('Delete temp folder', temp_out_dir)\n","print('Finish video', output_path)"],"metadata":{"id":"1i77RCTgIuWa"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"14gaKBtKzlB9lCipOvfkHccMAPfleW1-x","timestamp":1690476550017},{"file_id":"1hFtQeyHi99z9ZUvFOpdu3rlwbj58d0vY","timestamp":1688685840393}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}